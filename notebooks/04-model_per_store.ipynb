{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter, StrMethodFormatter\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('..')\n",
    "from src import config, dataset, models\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use(['ggplot', 'bmh'])\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "\n",
    "display_settings = {\n",
    "    'max_columns': 999,\n",
    "    'expand_frame_repr': True,\n",
    "    'max_rows': 999,\n",
    "    'precision': 4,\n",
    "    'show_dimensions': True\n",
    "}\n",
    "\n",
    "for op, value in display_settings.items():\n",
    "    pd.set_option(\"display.{}\".format(op), value)\n",
    "\n",
    "\n",
    "def fix_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "fix_seed(1234)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each store, an lgbm model is optimized. Inference is done for each store's ids and concatenated at the end.\n",
    "The idea is to allow more data to be used as the dataset is quite large, as well as to specialize models to specific stores and thus reduce variance of data they train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store: CA_1\n",
      "Load and filter store transactions...\n",
      "   dataset dim: (600391, 19)\n",
      "Prepare features...\n",
      "   dataset dim: (419709, 73)\n",
      "Prepare training, validation and test sets...\n",
      "   train set dim: (321132, 68)\n",
      "   val set dim: (80283, 68)\n",
      "   test set dim: (18294, 68)\n",
      "Train model...\n",
      "   loss: tweedie\n",
      "   trial 5 - best running loss: 3.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1368: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   trial 10 - best running loss: 3.843\n",
      "   trial 15 - best running loss: 3.843\n",
      "   best loss: 3.843\n",
      "Make predictions...\n",
      "\n",
      "\n",
      "Store: CA_2\n",
      "Load and filter store transactions...\n",
      "   dataset dim: (600240, 19)\n",
      "Prepare features...\n",
      "   dataset dim: (419407, 73)\n",
      "Prepare training, validation and test sets...\n",
      "   train set dim: (320890, 68)\n",
      "   val set dim: (80223, 68)\n",
      "   test set dim: (18294, 68)\n",
      "Train model...\n",
      "   loss: tweedie\n",
      "   trial 5 - best running loss: 3.894\n",
      "   trial 10 - best running loss: 3.892\n",
      "   trial 15 - best running loss: 3.888\n",
      "   trial 20 - best running loss: 3.888\n",
      "   best loss: 3.888\n",
      "Make predictions...\n",
      "\n",
      "\n",
      "Store: CA_3\n",
      "Load and filter store transactions...\n",
      "   dataset dim: (600328, 19)\n",
      "Prepare features...\n",
      "   dataset dim: (419414, 73)\n",
      "Prepare training, validation and test sets...\n",
      "   train set dim: (320896, 68)\n",
      "   val set dim: (80224, 68)\n",
      "   test set dim: (18294, 68)\n",
      "Train model...\n",
      "   loss: tweedie\n",
      "   trial 5 - best running loss: 4.547\n",
      "   trial 10 - best running loss: 4.547\n",
      "   trial 15 - best running loss: 4.542\n",
      "   best loss: 4.542\n",
      "Make predictions...\n",
      "\n",
      "\n",
      "Store: CA_4\n",
      "Load and filter store transactions...\n",
      "   dataset dim: (600295, 19)\n",
      "Prepare features...\n",
      "   dataset dim: (419768, 73)\n",
      "Prepare training, validation and test sets...\n",
      "   train set dim: (321179, 68)\n",
      "   val set dim: (80295, 68)\n",
      "   test set dim: (18294, 68)\n",
      "Train model...\n",
      "   loss: tweedie\n",
      "   trial 5 - best running loss: 2.980\n",
      "   trial 10 - best running loss: 2.980\n",
      "   trial 15 - best running loss: 2.980\n",
      "   trial 20 - best running loss: 2.980\n",
      "   trial 25 - best running loss: 2.980\n",
      "   trial 30 - best running loss: 2.980\n",
      "   best loss: 2.980\n",
      "Make predictions...\n",
      "\n",
      "\n",
      "Store: TX_1\n",
      "Load and filter store transactions...\n",
      "   dataset dim: (600626, 19)\n",
      "Prepare features...\n",
      "   dataset dim: (419930, 75)\n",
      "Prepare training, validation and test sets...\n",
      "   train set dim: (321308, 70)\n",
      "   val set dim: (80328, 70)\n",
      "   test set dim: (18294, 70)\n",
      "Train model...\n",
      "   loss: tweedie\n",
      "   trial 5 - best running loss: 3.196\n",
      "   trial 10 - best running loss: 3.196\n",
      "   trial 15 - best running loss: 3.196\n",
      "   best loss: 3.195\n",
      "Make predictions...\n",
      "\n",
      "\n",
      "Store: TX_2\n",
      "Load and filter store transactions...\n",
      "   dataset dim: (600550, 19)\n",
      "Prepare features...\n",
      "   dataset dim: (419930, 73)\n",
      "Prepare training, validation and test sets...\n",
      "   train set dim: (321308, 68)\n",
      "   val set dim: (80328, 68)\n",
      "   test set dim: (18294, 68)\n",
      "Train model...\n",
      "   loss: tweedie\n",
      "   trial 5 - best running loss: 3.592\n",
      "   trial 10 - best running loss: 3.591\n",
      "   trial 15 - best running loss: 3.591\n",
      "   trial 20 - best running loss: 3.591\n",
      "   best loss: 3.591\n",
      "Make predictions...\n",
      "\n",
      "\n",
      "Store: TX_3\n",
      "Load and filter store transactions...\n",
      "   dataset dim: (600626, 19)\n",
      "Prepare features...\n",
      "   dataset dim: (419916, 73)\n",
      "Prepare training, validation and test sets...\n",
      "   train set dim: (321297, 68)\n",
      "   val set dim: (80325, 68)\n",
      "   test set dim: (18294, 68)\n",
      "Train model...\n",
      "   loss: tweedie\n",
      "   trial 5 - best running loss: 3.467\n",
      "   trial 10 - best running loss: 3.461\n",
      "   trial 15 - best running loss: 3.460\n",
      "   best loss: 3.460\n",
      "Make predictions...\n",
      "\n",
      "\n",
      "Store: WI_1\n",
      "Load and filter store transactions...\n",
      "   dataset dim: (600556, 19)\n",
      "Prepare features...\n",
      "   dataset dim: (419763, 73)\n",
      "Prepare training, validation and test sets...\n",
      "   train set dim: (321175, 68)\n",
      "   val set dim: (80294, 68)\n",
      "   test set dim: (18294, 68)\n",
      "Train model...\n",
      "   loss: tweedie\n",
      "   trial 5 - best running loss: 3.664\n",
      "   trial 10 - best running loss: 3.654\n",
      "   trial 15 - best running loss: 3.652\n",
      "   trial 20 - best running loss: 3.652\n",
      "   trial 25 - best running loss: 3.652\n",
      "   best loss: 3.652\n",
      "Make predictions...\n",
      "\n",
      "\n",
      "Store: WI_2\n",
      "Load and filter store transactions...\n",
      "   dataset dim: (600397, 19)\n",
      "Prepare features...\n",
      "   dataset dim: (419643, 73)\n",
      "Prepare training, validation and test sets...\n",
      "   train set dim: (321079, 68)\n",
      "   val set dim: (80270, 68)\n",
      "   test set dim: (18294, 68)\n",
      "Train model...\n",
      "   loss: tweedie\n",
      "   trial 5 - best running loss: 3.848\n",
      "   trial 10 - best running loss: 3.847\n",
      "   trial 15 - best running loss: 3.847\n",
      "   trial 20 - best running loss: 3.847\n",
      "   best loss: 3.847\n",
      "Make predictions...\n",
      "\n",
      "\n",
      "Store: WI_3\n",
      "Load and filter store transactions...\n",
      "   dataset dim: (600653, 19)\n",
      "Prepare features...\n",
      "   dataset dim: (420038, 73)\n",
      "Prepare training, validation and test sets...\n",
      "   train set dim: (321395, 68)\n",
      "   val set dim: (80349, 68)\n",
      "   test set dim: (18294, 68)\n",
      "Train model...\n",
      "   loss: tweedie\n",
      "   trial 5 - best running loss: 3.405\n",
      "   trial 10 - best running loss: 3.401\n",
      "   trial 15 - best running loss: 3.400\n",
      "   trial 20 - best running loss: 3.400\n",
      "   trial 25 - best running loss: 3.400\n",
      "   best loss: 3.400\n",
      "Make predictions...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_1_validation</td>\n",
       "      <td>0.5195</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.4942</td>\n",
       "      <td>0.5041</td>\n",
       "      <td>0.5618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_002_CA_1_validation</td>\n",
       "      <td>0.3291</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.2942</td>\n",
       "      <td>0.3031</td>\n",
       "      <td>0.2792</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_003_CA_1_validation</td>\n",
       "      <td>0.6010</td>\n",
       "      <td>0.4818</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>0.5017</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.5900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_004_CA_1_validation</td>\n",
       "      <td>1.0281</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>0.9389</td>\n",
       "      <td>1.0656</td>\n",
       "      <td>1.0442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_005_CA_1_validation</td>\n",
       "      <td>1.0131</td>\n",
       "      <td>0.7899</td>\n",
       "      <td>0.8097</td>\n",
       "      <td>0.8553</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>1.1880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id      F1      F2      F3      F4      F5      F6\n",
       "0  FOODS_1_001_CA_1_validation  0.5195  0.4767  0.4846  0.4942  0.5041  0.5618\n",
       "1  FOODS_1_002_CA_1_validation  0.3291  0.2948  0.2942  0.3031  0.2792  0.3333\n",
       "2  FOODS_1_003_CA_1_validation  0.6010  0.4818  0.4956  0.5017  0.5525  0.5900\n",
       "3  FOODS_1_004_CA_1_validation  1.0281  0.9003  0.9170  0.9389  1.0656  1.0442\n",
       "4  FOODS_1_005_CA_1_validation  1.0131  0.7899  0.8097  0.8553  0.9966  1.1880\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = config.get_processed_filename('data_merged.parquet')\n",
    "stores = config.STORES\n",
    "categories = ['item_id', 'dept_id', 'cat_id', 'wday', 'month', 'snap']\n",
    "\n",
    "# prediction horizon\n",
    "horizon = 28\n",
    "\n",
    "# The dataset is quite large with 46 million records. We use only a fraction of it for training.\n",
    "last_day = 1913\n",
    "start_day = last_day - 6 * horizon\n",
    "\n",
    "losses = {}\n",
    "df_predictions = pd.DataFrame()\n",
    "\n",
    "#stores = ['CA_1']\n",
    "\n",
    "for store in stores:\n",
    "    print(f'Store: {store}')\n",
    "    print('Load and filter store transactions...')\n",
    "    # filter\n",
    "    df_train = pd.read_parquet(path)\n",
    "    df_train = df_train[df_train.store_id == store]\n",
    "    df_train = df_train.drop('store_id', axis=1)\n",
    "    df_train = df_train[df_train.day_ind >=start_day]\n",
    "    \n",
    "    print(f'   dataset dim: {df_train.shape}')\n",
    "    \n",
    "    print('Prepare features...')\n",
    "    # convert categories to ordinal\n",
    "    df_train = dataset.categorical_to_ordinal(df_train)\n",
    "    df_train = df_train.sort_values('date')\n",
    "    \n",
    "    # time dependent features\n",
    "    for c in categories:\n",
    "        df_train = dataset.hierarchy_stats(df_train, c, ['sold_qty', 'sell_price'])\n",
    "\n",
    "        df_grouped = df_train.groupby([c, 'date'])\n",
    "        df_train = dataset.hierarchy_rolling_mean(df_train, c, ['sold_qty', 'sell_price'], window=7, shift=28, df_grouped=df_grouped)\n",
    "        df_train = dataset.hierarchy_rolling_std(df_train, c, ['sold_qty', 'sell_price'], window=7, shift=28, df_grouped=df_grouped)\n",
    "        df_train = dataset.hierarchy_rolling_mean(df_train, c, ['sold_qty', 'sell_price'], window=28, shift=28, df_grouped=df_grouped)\n",
    "        df_train = dataset.hierarchy_rolling_std(df_train, c, ['sold_qty', 'sell_price'], window=28, shift=28, df_grouped=df_grouped)\n",
    "\n",
    "    # lag per id\n",
    "    df_grouped = df_train.groupby('id', as_index=False)\n",
    "    df_train = dataset.item_lag(df_train, ['sold_qty'], n_most=2, after=28, df_grouped=df_grouped)\n",
    "    \n",
    "    # exclude rows with nan due to lagged features\n",
    "    df_train.dropna(subset=[c for c in df_train.columns if c != 'sold_qty'], inplace=True)\n",
    "    \n",
    "    # remove columns with zero variance\n",
    "    col_std = df_train[df_train.select_dtypes([np.number]).columns].std()\n",
    "    cols_to_drop = col_std[col_std == 0].index\n",
    "    df_train.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    del df_grouped, col_std, cols_to_drop\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'   dataset dim: {df_train.shape}')\n",
    "    \n",
    "    print('Prepare training, validation and test sets...')\n",
    "    # prepare training and test set\n",
    "    for c in categories:\n",
    "        df_train[c] = df_train[c].astype('category')\n",
    "\n",
    "    # drop useless columns\n",
    "    useless_columns = ['id', 'day_ind', 'date', 'event_type_1']\n",
    "\n",
    "    # test set\n",
    "    test_ids = df_train.loc[df_train.sold_qty.isna(), 'id']\n",
    "    test_day_ind = df_train.loc[df_train.sold_qty.isna(), 'day_ind']\n",
    "    X_test = df_train.loc[df_train.sold_qty.isna()].drop(useless_columns + ['sold_qty'], axis=1)\n",
    "    \n",
    "    # train set\n",
    "    y_train = df_train.loc[~df_train.sold_qty.isna(), 'sold_qty']\n",
    "    X_train = df_train.loc[~df_train.sold_qty.isna()].drop(useless_columns + ['sold_qty'], axis=1)\n",
    "    \n",
    "    del df_train\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'   day {df_train.day_ind.min()} to day {df_train.day_ind.max()}')\n",
    "    print(f'   train set dim: {X_train.shape}')\n",
    "    print(f'   test set dim: {X_test.shape}')\n",
    "    \n",
    "    print('Train model...')\n",
    "    # train model\n",
    "    \n",
    "    model = models.LGBModel()\n",
    "    model.optimize(X_train, y_train, n_trials=15, metric='mape')\n",
    "    \n",
    "    # evaluate\n",
    "    loss = model.best_value\n",
    "    losses[store] = loss\n",
    "    \n",
    "    print('Make predictions...')\n",
    "    # predictions on test set\n",
    "    y_hat_test = model.predict(X_test)\n",
    "    df_predictions_store = pd.DataFrame({'id': test_ids, 'd': test_day_ind, 'sold_qty': y_hat_test})\n",
    "    \n",
    "    # transform to the required format for submissions\n",
    "    df_predictions_store = df_predictions_store.pivot(index='id', columns='d', values='sold_qty')\n",
    "    df_predictions_store.columns = [f'F{i + 1}' for i, c in enumerate(df_predictions_store.columns)]\n",
    "    df_predictions_store.reset_index(inplace=True)\n",
    "    \n",
    "    df_predictions = pd.concat([df_predictions, df_predictions_store], axis=0)\n",
    "    \n",
    "    del X_train, X_val, y_train, y_val, df_predictions_store\n",
    "    gc.collect()\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "# duplicate and append predictions for evaluation period, as validation period is not known yet\n",
    "df_predictions_eval = df_predictions.copy()\n",
    "df_predictions_eval.id = df_predictions_eval.id.str.replace('validation', 'evaluation')\n",
    "\n",
    "df_predictions = pd.concat([df_predictions, df_predictions_eval], axis=0).reset_index(drop=True)\n",
    "del df_predictions_eval\n",
    "\n",
    "\n",
    "# save to directory\n",
    "path = config.get_submission_filename('submit_model_per_store.csv')\n",
    "df_predictions.to_csv(path, index=False)\n",
    "\n",
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
